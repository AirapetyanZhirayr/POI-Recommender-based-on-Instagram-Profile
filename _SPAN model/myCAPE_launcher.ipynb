{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SdLsnYOcn-PY"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import torch\n",
    "from torch import nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VOmGMTrwoBYG"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/df_cutted.csv')\n",
    "\n",
    "with open('../data/locations_c.txt', 'r') as json_file:\n",
    "    location_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "rkWdH8c7qjhK"
   },
   "outputs": [],
   "source": [
    "# constructing POI data\n",
    "location_data_cleaned = defaultdict(dict)\n",
    "for loc in location_data:\n",
    "    z = location_data[loc]\n",
    "    key = z['city'] + ', ' + z['country']\n",
    "    value = {'lat':z['lat'], 'lng':z['lng']}\n",
    "    location_data_cleaned[key] = value\n",
    "location_data = location_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iomawFJ6oKpR"
   },
   "outputs": [],
   "source": [
    "def indexate(data):\n",
    "  data2id = {'<PAD>':0}\n",
    "  id2data = ['<PAD>']\n",
    "  for el in data:\n",
    "    if el is np.nan:\n",
    "      continue\n",
    "    if el not in data2id:\n",
    "      data2id[el] = len(data2id)\n",
    "      id2data.append(el)\n",
    "  return data2id, id2data\n",
    "\n",
    "user2id, id2user = indexate(df['user'])\n",
    "poi2id, id2poi = indexate(df['location'])\n",
    "n_poi = len(poi2id) - 1\n",
    "n_users = len(user2id) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6x4L_K9ps4kF",
    "outputId": "69ea1a55-8ac2-44d0-d727-fe3e0ef5aefb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "147641it [00:12, 12235.99it/s]\n"
     ]
    }
   ],
   "source": [
    "POI_data = np.zeros(shape=(n_poi, 3))\n",
    "for poi, idx in poi2id.items():\n",
    "  if idx == 0:\n",
    "    continue\n",
    "  lat = location_data[poi]['lat']\n",
    "  lng = location_data[poi]['lng']\n",
    "  POI_data[idx-1] = np.array([idx, lat, lng])  \n",
    "\n",
    "user_data = []\n",
    "for _, row in tqdm(df[~df['location'].isna()].iterrows()):\n",
    "  user_id = user2id[row['user']]\n",
    "  poi_id = poi2id[row['location']]\n",
    "  timestamp = row['timestamp']\n",
    "  user_data.append([user_id, poi_id, timestamp])\n",
    "user_data = np.array(user_data).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cG0fYqfywk0-"
   },
   "outputs": [],
   "source": [
    "\n",
    "dname='instgrm'\n",
    "np.save(f'../data/{dname}.npy', user_data)\n",
    "np.save(f'../data/{dname}_POI.npy', POI_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aG5Ux3IvyGuI"
   },
   "outputs": [],
   "source": [
    "from load import *\n",
    "from layers import *\n",
    "from models import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ziA0PL8iyRCs"
   },
   "outputs": [],
   "source": [
    "process_traj(dname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "m5TdlryZ_sIU"
   },
   "outputs": [],
   "source": [
    "load=True\n",
    "\n",
    "def calculate_acc(prob, label):\n",
    "    # log_prob (N, L), label (N), batch_size [*M]\n",
    "    acc_train = [0, 0, 0, 0]\n",
    "    for i, k in enumerate([1, 5, 10, 20]):\n",
    "        # topk_batch (N, k)\n",
    "        _, topk_predict_batch = torch.topk(prob, k=k)\n",
    "        for j, topk_predict in enumerate(to_npy(topk_predict_batch)):\n",
    "            # topk_predict (k)\n",
    "            if to_npy(label)[j] in topk_predict:\n",
    "                acc_train[i] += 1\n",
    "\n",
    "    return np.array(acc_train)\n",
    "\n",
    "\n",
    "def sampling_prob(prob, label, num_neg):\n",
    "    num_label, l_m = prob.shape[0], prob.shape[1]-1  # prob (N, L)\n",
    "    label = label.view(-1)  # label (N)\n",
    "    init_label = np.linspace(0, num_label-1, num_label)  # (N), [0 -- num_label-1]\n",
    "    init_prob = torch.zeros(size=(num_label, num_neg+len(label)))  # (N, num_neg+num_label)\n",
    "\n",
    "    random_ig = random.sample(range(1, l_m+1), num_neg)  # (num_neg) from (1 -- l_max)\n",
    "    while len([lab for lab in label if lab in random_ig]) != 0:  # no intersection\n",
    "        random_ig = random.sample(range(1, l_m+1), num_neg)\n",
    "\n",
    "    global global_seed\n",
    "    random.seed(global_seed)\n",
    "    global_seed += 1\n",
    "\n",
    "    # place the pos labels ahead and neg samples in the end\n",
    "    for k in range(num_label):\n",
    "        for i in range(num_neg + len(label)):\n",
    "            if i < len(label):\n",
    "                init_prob[k, i] = prob[k, label[i]]\n",
    "            else:\n",
    "                init_prob[k, i] = prob[k, random_ig[i-len(label)]]\n",
    "\n",
    "    return torch.FloatTensor(init_prob), torch.LongTensor(init_label)  # (N, num_neg+num_label), (N)\n",
    "\n",
    "\n",
    "class DataSet(data.Dataset):\n",
    "    def __init__(self, traj, m1, v, label, length):\n",
    "        # (NUM, M, 3), (NUM, M, M, 2), (L, L), (NUM, M), (NUM), (NUM)\n",
    "        self.traj, self.mat1, self.vec, self.label, self.length = traj, m1, v, label, length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        traj = self.traj[index].to(device)\n",
    "        mats1 = self.mat1[index].to(device)\n",
    "        vector = self.vec[index].to(device)\n",
    "        label = self.label[index].to(device)\n",
    "        length = self.length[index].to(device)\n",
    "        return traj, mats1, vector, label, length\n",
    "\n",
    "    def __len__(self):  # no use\n",
    "        return len(self.traj)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, record):\n",
    "        # load other parameters\n",
    "        self.model = model.to(device)\n",
    "        self.records = record\n",
    "        self.start_epoch = record['epoch'][-1] if load else 1\n",
    "        self.num_neg = 10\n",
    "        self.interval = 1000\n",
    "        self.batch_size = 1 # N = 1\n",
    "        self.learning_rate = 3e-3\n",
    "        self.num_epoch = 10\n",
    "        self.threshold = np.mean(record['acc_valid'][-1]) if load else 0  # 0 if not update\n",
    "\n",
    "        # (NUM, M, 3), (NUM, M, M, 2), (L, L), (NUM, M, M), (NUM, M), (NUM) i.e. [*M]\n",
    "        self.traj, self.mat1, self.mat2s, self.mat2t, self.label, self.len = \\\n",
    "            trajs, mat1, mat2s, mat2t, labels, lens\n",
    "        # nn.cross_entropy_loss counts target from 0 to C - 1, so we minus 1 here.\n",
    "        self.dataset = DataSet(self.traj, self.mat1, self.mat2t, self.label-1, self.len)\n",
    "        self.data_loader = data.DataLoader(dataset=self.dataset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def train(self):\n",
    "        # set optimizer\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=0)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=1)\n",
    "\n",
    "        for t in range(self.num_epoch):\n",
    "            # settings or validation and test\n",
    "            valid_size, test_size = 0, 0\n",
    "            acc_valid, acc_test = [0, 0, 0, 0], [0, 0, 0, 0]\n",
    "\n",
    "            bar = tqdm(total=part)\n",
    "            for step, item in enumerate(self.data_loader):\n",
    "                # get batch data, (N, M, 3), (N, M, M, 2), (N, M, M), (N, M), (N)\n",
    "                person_input, person_m1, person_m2t, person_label, person_traj_len = item\n",
    "\n",
    "                # first, try batch_size = 1 and mini_batch = 1\n",
    "\n",
    "                input_mask = torch.zeros((self.batch_size, max_len, 3), dtype=torch.long).to(device)\n",
    "                m1_mask = torch.zeros((self.batch_size, max_len, max_len, 2), dtype=torch.float32).to(device)\n",
    "                for mask_len in range(1, person_traj_len[0]+1):  # from 1 -> len\n",
    "                    # if mask_len != person_traj_len[0]:\n",
    "                    #     continue\n",
    "                    input_mask[:, :mask_len] = 1.\n",
    "                    m1_mask[:, :mask_len, :mask_len] = 1.\n",
    "\n",
    "                    train_input = person_input * input_mask\n",
    "                    train_m1 = person_m1 * m1_mask\n",
    "                    train_m2t = person_m2t[:, mask_len - 1]\n",
    "                    train_label = person_label[:, mask_len - 1]  # (N)\n",
    "                    train_len = torch.zeros(size=(self.batch_size,), dtype=torch.long).to(device) + mask_len\n",
    "\n",
    "                    prob = self.model(train_input, train_m1, self.mat2s, train_m2t, train_len)  # (N, L)\n",
    "\n",
    "                    if mask_len <= person_traj_len[0] - 2:  # only training\n",
    "                        # nn.utils.clip_grad_norm_(self.model.parameters(), 10)\n",
    "                        prob_sample, label_sample = sampling_prob(prob, train_label, self.num_neg)\n",
    "                        loss_train = F.cross_entropy(prob_sample, label_sample)\n",
    "                        loss_train.backward()\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                        scheduler.step()\n",
    "\n",
    "                    elif mask_len == person_traj_len[0] - 1:  # only validation\n",
    "                        valid_size += person_input.shape[0]\n",
    "                        # v_prob_sample, v_label_sample = sampling_prob(prob_valid, valid_label, self.num_neg)\n",
    "                        # loss_valid += F.cross_entropy(v_prob_sample, v_label_sample, reduction='sum')\n",
    "                        acc_valid += calculate_acc(prob, train_label)\n",
    "\n",
    "                    elif mask_len == person_traj_len[0]:  # only test\n",
    "                        test_size += person_input.shape[0]\n",
    "                        # v_prob_sample, v_label_sample = sampling_prob(prob_valid, valid_label, self.num_neg)\n",
    "                        # loss_valid += F.cross_entropy(v_prob_sample, v_label_sample, reduction='sum')\n",
    "                        acc_test += calculate_acc(prob, train_label)\n",
    "\n",
    "                bar.update(self.batch_size)\n",
    "            bar.close()\n",
    "\n",
    "            acc_valid = np.array(acc_valid) / valid_size\n",
    "            print('epoch:{}, time:{}, valid_acc:{}'.format(self.start_epoch + t, time.time() - start, acc_valid))\n",
    "\n",
    "            acc_test = np.array(acc_test) / test_size\n",
    "            print('epoch:{}, time:{}, test_acc:{}'.format(self.start_epoch + t, time.time() - start, acc_test))\n",
    "\n",
    "            self.records['acc_valid'].append(acc_valid)\n",
    "            self.records['acc_test'].append(acc_test)\n",
    "            self.records['epoch'].append(self.start_epoch + t)\n",
    "\n",
    "            if self.threshold < np.mean(acc_valid):\n",
    "                self.threshold = np.mean(acc_valid)\n",
    "                # save the model\n",
    "                torch.save({'state_dict': self.model.state_dict(),\n",
    "                            'records': self.records,\n",
    "                            'time': time.time() - start},\n",
    "                           'best_stan_' + dname + '.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "APN_gmkF7pNA"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "file = open('./data/' + dname + '_data.pkl', 'rb')\n",
    "file_data = joblib.load(file)\n",
    "# tensor(NUM, M, 3), np(NUM, M, M, 2), np(L, L), np(NUM, M, M), tensor(NUM, M), np(NUM)\n",
    "[trajs, mat1, mat2s, mat2t, labels, lens, u_max, l_max] = file_data\n",
    "mat1, mat2s, mat2t, lens = torch.FloatTensor(mat1), torch.FloatTensor(mat2s).to(device), \\\n",
    "                            torch.FloatTensor(mat2t), torch.LongTensor(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wk8Anth38Z4m",
    "outputId": "651384e1-d5a1-477f-c8c9-152665d4660f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(19971.1660), tensor(0.), tensor(9.2234e+18), tensor(0.))"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part = len(trajs)\n",
    "ex = mat1[:, :, :, 0].max(), mat1[:, :, :, 0].min(), mat1[:, :, :, 1].max(), mat1[:, :, :, 1].min()\n",
    "ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-Wtrh1gq8hAa"
   },
   "outputs": [],
   "source": [
    "stan = Model(t_dim=hours+1, l_dim=l_max+1, u_dim=u_max+1, embed_dim=10, ex=ex, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGsKcFWT-pgF",
    "outputId": "d6acc66f-6560-4f69-8577-8a4bd1576b95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiEmbed.emb_t.weight\n",
      "MultiEmbed.emb_l.weight\n",
      "MultiEmbed.emb_u.weight\n",
      "MultiEmbed.emb_su.weight\n",
      "MultiEmbed.emb_sl.weight\n",
      "MultiEmbed.emb_tu.weight\n",
      "MultiEmbed.emb_tl.weight\n",
      "SelfAttn.query.weight\n",
      "SelfAttn.key.weight\n",
      "SelfAttn.value.weight\n",
      "Embed.emb_su.weight\n",
      "Embed.emb_sl.weight\n",
      "Embed.emb_tu.weight\n",
      "Embed.emb_tl.weight\n",
      "Attn.value.weight\n",
      "Attn.emb_loc.weight\n",
      "num of params 202575\n"
     ]
    }
   ],
   "source": [
    "num_params = 0\n",
    "\n",
    "for name in stan.state_dict():\n",
    "    print(name)\n",
    "\n",
    "for param in stan.parameters():\n",
    "    num_params += param.numel()\n",
    "print('num of params', num_params)\n",
    "\n",
    "load = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "9JC3ciZq-wWo",
    "outputId": "a92316b8-386f-4123-c593-8b481256f71e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16275/16275 [11:12<00:00, 24.20it/s] \n",
      "  0%|          | 1/16275 [00:00<28:34,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:29, time:20098.81499528885, valid_acc:[0.12746555 0.31565793 0.4167117  0.51857606]\n",
      "epoch:29, time:20098.815460205078, test_acc:[0.11469213 0.289934   0.38367399 0.4832447 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1293/16275 [01:31<17:09, 14.56it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-202f93e24a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-d852f6aae3d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                         \u001b[0mprob_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampling_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if load:\n",
    "    checkpoint = torch.load('best_stan_' + dname + '.pth')\n",
    "    stan.load_state_dict(checkpoint['state_dict'])\n",
    "    start = time.time() - checkpoint['time']\n",
    "    records = checkpoint['records']\n",
    "else:\n",
    "    records = {'epoch': [], 'acc_valid': [], 'acc_test': []}\n",
    "    start = time.time()\n",
    "\n",
    "trainer = Trainer(stan, records)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "myCAPE_launcher.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
